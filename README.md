# ğŸš€ Awesome LLMSecOps

<div align="center">

[![Awesome](https://awesome.re/badge-flat2.svg)](https://awesome.re)
![GitHub stars](https://img.shields.io/github/stars/wearetyomsmnv/awesome-llmsecops?style=flat-square&color=yellow)
![GitHub forks](https://img.shields.io/github/forks/wearetyomsmnv/awesome-llmsecops?style=flat-square&color=blue)
![GitHub last commit](https://img.shields.io/github/last-commit/wearetyomsmnv/awesome-llmsecops?style=flat-square&color=green)

ğŸ” A curated list of awesome resources for LLMSecOps (Large Language Model Security Operations) ğŸ§ 

[Introduction](#introduction) â€¢ [Tools](#tools) â€¢ [Frameworks](#frameworks) â€¢ [Best Practices](#best-practices) â€¢ [Research](#research) â€¢ [Tutorials](#tutorials) â€¢ [Community](#community)

</div>

---

## ğŸ“š Introduction

LLM safety is a huge body of knowledge that is important and relevant to society today. The purpose of this Awesome list is to provide the community with the necessary knowledge on how to build an LLM development process - safe, as well as what threats may be encountered along the way. Everyone is welcome to contribute. 

This repository, unlike many existing repositories, emphasizes the practical implementation of security and does not provide a lot of references to arxiv in the description.

---

## ğŸ›  Tools

| Tool | Description | Stars |
|------|-------------|-------|
| [ğŸ”§ Garak](https://github.com/leondz/garak) | LLM vulnerability scanner | ![GitHub stars](https://img.shields.io/github/stars/leondz/garak?style=social) |
| [ğŸ”§ ps-fuzz 2](https://github.com/prompt-security/ps-fuzz) | Make your GenAI Apps Safe & Secure ğŸš€ Test & harden your system prompt | ![GitHub stars](https://img.shields.io/github/stars/prompt-security/ps-fuzz?style=social) |




| Name | LLM Security Capabilities | URL |
|------|---------------------------|-----|
| CalypsoAI Moderator | Focuses on preventing data leakage, full auditability, and malicious code detection. | https://www.prompt.security |
| Giskard | AI quality management system for ML models, focusing on vulnerabilities such as performance bias, hallucinations, and prompt injections. | https://www.giskard.ai/ |
| Lakera | Lakera Guard enhances LLM application security and counters a wide range of AI cyber threats. | https://www.lakera.ai/ |
| Lasso Security | Focuses on LLMs, offering security assessment, advanced threat modeling, and specialized training programs. | https://www.lasso.security/ |
| LLM Guard | Designed to strengthen LLM security, offers sanitization, malicious language detection, data leak prevention, and prompt injection resilience. | https://llmguard.com |
| LLM Fuzzer | Open-source fuzzing framework specifically designed for LLMs, focusing on integration into applications via LLM APIs. | https://github.com/llmfuzzer |
| Prompt Security | Provides a security, data privacy, and safety approach across all aspects of generative AI, independent of specific LLMs. | https://www.prompt.security |
| Rebuff | Self-hardening prompt injection detector for AI applications, using a multi-layered protection mechanism. | https://github.com/rebuff |
| Robust Intelligence | Provides AI firewall and continuous testing and evaluation. Creators of the airisk.io database donated to MITRE. | https://www.whylabs.ai/ |
| WhyLabs | Protects LLMs from security threats, focusing on data leak prevention, prompt injection monitoring, and misinformation prevention. | https://www.whylabs.ai/


## DATA

[Safety and privacy with Large Language Models](https://github.com/annjawn/llm-safety-privacy) 

## ğŸ— Frameworks

<table>
  <tr>
    <td align="center"><a href="https://owasp.org/www-project-top-10-for-large-language-model-applications/"><img src="https://owasp.org/assets/images/logo.png" width="100px;" alt=""/><br /><sub><b>OWASP LLM TOP 10</b></sub></a><br />10 vulnerabilities for llm</td>
    <td align="center"><a href="https://owasp.org/www-project-top-10-for-large-language-model-applications/llm-top-10-governance-doc/LLM_AI_Security_and_Governance_Checklist-v1.pdf"><img src="https://owasp.org/assets/images/logo.png" width="100px;" alt=""/><br /><sub><b>LLM AI Cybersecurity & Governance Checklist 2</b></sub></a><br />Brief explanation</td>
  </tr>
</table>

---

## ğŸ’¡ Best Practices

<details>
<summary>OWASP LLMSVS</summary>
OWASP LLMSVS - https://owasp.org/www-project-llm-verification-standard/
</details>


---


## ğŸ“Š Research Papers

| Title | Authors | Year | Citations |
|-------|---------|------|-----------|
| [ğŸ“„ Bypassing Metaâ€™s LLaMA Classifier: A Simple Jailbreak](https://www.robustintelligence.com/blog-posts/bypassing-metas-llama-classifier-a-simple-jailbreak) | Robust Intelligence | 2024 |
| [ğŸ“„ Vulnerabilities in LangChain Gen AI](https://unit42.paloaltonetworks.com/langchain-vulnerabilities/) | Unit42 | 2024 |

---

## ğŸ“ Tutorials

1.  [ğŸ“š HADESS - Web LLM Attacks](https://hadess.io/web-llm-attacks/)
   - Understand how u can do attack in web via llm
2. [ğŸ“š Red Teaming with LLMs](https://redteamrecipe.com/red-teaming-with-llms)
   - Practical Techniques for attacking ai systems
3. [ğŸ“š Lakera LLM Security](https://www.lakera.ai/blog/llm-security)
   - Overwiev for attacks on llm

---

## ğŸŒ Community

<div align="left">

- [OWASP SLACK](https://owasp.org/slack/invite)    
Channels:
    - #project-top10-for-llm
    - #ml-risk-top5
    - #project-ai-community
    - #project-mlsec-top10
    - #team-llm_ai-secgov
    - #team-llm-redteam
    - #team-llm-v2-brainstorm
    
- [Awesome LLM Security](https://github.com/corca-ai/awesome-llm-security)
- [PWNAI](https://t.me/pwnai)
- [AiSec_X_Feed](https://t.me/aisecnews)
- [LVE_Project](https://lve-project.org/)
- [Lakera AI Security resource hub](https://docs.google.com/spreadsheets/d/1tv3d2M4-RO8xJYiXp5uVvrvGWffM-40La18G_uFZlRM/edit?gid=639798153#gid=639798153)


</div>
